# JaxNetwork
## Practice with implementing trainable NNs in python using Jax. 

The following is very rough flexible implementation of NN using Jax. This jax implementation allows the user to put together a fully connected network with an arbitrary number of layers and neurons. Jax is used to calculate the gradient of the loss function. 

"Basic Jax Network" is a jupyter notebook with a very simplistic implementation of a network. In the example a network with an input layer, 2 hidden layer and a output layer is implementated. The network takes only 2 inputs, and outputs a value between 0 and 1. The network is meant to learn only the simple rule of any logic gate, including OR, AND, XOR, XNOR etc. 


### Notes: 
----

- The optimizer used in the basic implementation is SGD. 


